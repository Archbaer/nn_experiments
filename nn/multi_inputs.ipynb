{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423a8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "import pandas as pd # We'll use pandas to read in the data and normalize it\n",
    "from sklearn.model_selection import train_test_split # train_test_split will help us split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bd81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=2, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2, out_features=3, bias=True)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.fc1(input)\n",
    "        output_values = self.fc2(F.relu(hidden))\n",
    "        \n",
    "        return output_values\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        output_values = self.forward(inputs)\n",
    "        loss = self.loss(output_values, labels)\n",
    "                \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95423846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "class MultiInOut2(L.LightningModule):\n",
    "\n",
    "    L.seed_everything(42)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=2, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2, out_features=3, bias=True)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.fc1(input)\n",
    "        logits = self.fc2(F.relu(hidden))\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        output_values = self.forward(inputs)\n",
    "        loss = self.loss(output_values, labels)\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8425a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40232633",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = torch.tensor(iris.data, dtype=torch.float32)\n",
    "y = torch.tensor(iris.target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef5738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db6f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = TensorDataset(X_train, y_train)\n",
    "iris_loader = DataLoader(iris_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ead481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "model = MultiInOut2()\n",
    "trainer = L.Trainer(max_epochs=100, accelerator=\"cpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "001936e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 10     | train\n",
      "1 | fc2  | Linear           | 9      | train\n",
      "2 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 500.01it/s, v_num=35]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 248.03it/s, v_num=35]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, iris_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe21bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight tensor([[ 0.3036,  0.3176, -0.1817,  0.3952],\n",
      "        [-0.1096,  0.1009, -0.2434,  0.2936]])\n",
      "fc1.bias tensor([ 0.3556, -0.3668])\n",
      "fc2.weight tensor([[ 0.4407,  0.1323],\n",
      "        [ 0.4834,  0.0958],\n",
      "        [ 0.5372, -0.0998]])\n",
      "fc2.bias tensor([ 0.3268,  0.1346, -0.1055])\n",
      "tensor([[ 0.3036,  0.3176, -0.1817,  0.3952],\n",
      "        [-0.1096,  0.1009, -0.2434,  0.2936]])\n",
      "fc1.bias tensor([ 0.3556, -0.3668])\n",
      "fc2.weight tensor([[ 0.4407,  0.1323],\n",
      "        [ 0.4834,  0.0958],\n",
      "        [ 0.5372, -0.0998]])\n",
      "fc2.bias tensor([ 0.3268,  0.1346, -0.1055])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8fdc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "pred = model(X_test)\n",
    "predicted_classes = torch.argmax(pred, dim=1)\n",
    "accuracy = (predicted_classes == y_test).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aac9fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2452725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer1 = L.Trainer(max_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01b95518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_35/checkpoints/epoch=99-step=400.ckpt\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:366: The dirpath has changed from '/Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_35/checkpoints' to '/Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_36/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 10     | train\n",
      "1 | fc2  | Linear           | 9      | train\n",
      "2 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:366: The dirpath has changed from '/Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_35/checkpoints' to '/Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_36/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 10     | train\n",
      "1 | fc2  | Linear           | 9      | train\n",
      "2 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_35/checkpoints/epoch=99-step=400.ckpt\n",
      "Restored all states from the checkpoint at /Users/archbaer/projects/nn_experiments/nn/lightning_logs/version_35/checkpoints/epoch=99-step=400.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 213.07it/s, v_num=36]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 158.75it/s, v_num=36]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer1.fit(model, iris_loader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f5f810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(X_test)\n",
    "predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "torch.sum(torch.eq(predicted_classes, y_test)).item() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45284f",
   "metadata": {},
   "source": [
    "* Now let's try with a modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cb9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/StatQuest/signa/main/chapter_04/iris.txt\"\n",
    "df = pd.read_table(url, sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f99d128d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3            4\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c4b1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"sepal_length\",\n",
    "              \"sepal_width\",\n",
    "              \"petal_length\",\n",
    "              \"petal_width\",\n",
    "              \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d78b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = df[[\"sepal_length\", \"sepal_width\"]]\n",
    "label_values = df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446e7c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_as_numbers = label_values.factorize()[0] ## NOTE: factorize() returns a list of lists,\n",
    "                                                 ## and since we only need the first list of values,\n",
    "                                                 ## we index the output of factorize() with [0].\n",
    "classes_as_numbers ## print out the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ba872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, label_train, label_test = train_test_split(input_values,\n",
    "                                                                    classes_as_numbers,\n",
    "                                                                    test_size=0.25,\n",
    "                                                                    stratify=classes_as_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78041a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_label_train = F.one_hot(torch.tensor(label_train)).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "481480e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_label_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f22d523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "input_train_scaled = scaler.fit_transform(input_train)\n",
    "input_test_scaled = scaler.transform(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfba56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_input_train = torch.tensor(input_train_scaled, dtype=torch.float32)\n",
    "tensor_input_test = torch.tensor(input_test_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6b72bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(tensor_input_train, one_hot_label_train)\n",
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eebe30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(accelerator=\"cpu\", devices=1, max_epochs=100)\n",
    "model = MultiInputModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1d132ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 6      | train\n",
      "1 | fc2  | Linear           | 9      | train\n",
      "2 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "15        Trainable params\n",
      "0         Non-trainable params\n",
      "15        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "3         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/opt/anaconda3/envs/nn/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 372.97it/s, v_num=37]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 369.00it/s, v_num=37]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "004ec0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.58%\n"
     ]
    }
   ],
   "source": [
    "predictions = model(tensor_input_test) \n",
    "predicted_classes = torch.argmax(predictions, dim=1)  \n",
    "\n",
    "# To compare with true labels and get accuracy:\n",
    "accuracy = (predicted_classes == torch.tensor(label_test)).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=2, out_features=8, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=8, out_features=16, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=32, bias=True)\n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=3, bias=True)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.fc1(input)\n",
    "        hidden2 = self.fc2(F.relu(hidden))\n",
    "        hidden3 = self.fc3(F.relu(hidden2))\n",
    "        output_values = self.fc4(F.relu(hidden3))\n",
    "\n",
    "        return output_values\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        output_values = self.forward(inputs)\n",
    "        loss = self.loss(output_values, labels)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669301d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1070b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 24     | train\n",
      "1 | fc2  | Linear           | 144    | train\n",
      "2 | fc3  | Linear           | 544    | train\n",
      "3 | fc4  | Linear           | 99     | train\n",
      "4 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "811       Trainable params\n",
      "0         Non-trainable params\n",
      "811       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | fc1  | Linear           | 24     | train\n",
      "1 | fc2  | Linear           | 144    | train\n",
      "2 | fc3  | Linear           | 544    | train\n",
      "3 | fc4  | Linear           | 99     | train\n",
      "4 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "811       Trainable params\n",
      "0         Non-trainable params\n",
      "811       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 322.23it/s, v_num=38]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 318.81it/s, v_num=38]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultiInputModel()\n",
    "trainer = L.Trainer(accelerator=\"cpu\", devices=1, max_epochs=100)\n",
    "trainer.fit(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80d1927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 78.95%\n"
     ]
    }
   ],
   "source": [
    "predictions = model(tensor_input_test)  \n",
    "predicted_classes = torch.argmax(predictions, dim=1) \n",
    "\n",
    "# To compare with true labels and get accuracy:\n",
    "accuracy = (predicted_classes == torch.tensor(label_test)).float().mean()\n",
    "print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacdea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
