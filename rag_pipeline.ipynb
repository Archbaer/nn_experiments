{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3a13b9-352e-4f7e-a082-37a4f175105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyMuPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3721b227-fb3b-4750-9cb3-bd94a3035795",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84614305-271f-4f8f-a5f0-76fbd58113d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 28 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path=\"./Bookshelf\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068538d2-9fd7-4db7-bb9a-92db0f6e6bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bead21-c994-42ba-8ccb-91de2d1b4d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21584"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c945958e-74c3-45c5-88a9-388a5547ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25f5d94-da59-4283-aae2-19d0393ac8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130a5dcf-b8ac-43ba-8a8e-57bae61611fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOllama(model=\"deepseek-r1\", temperature=0.0)\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96a57dd-931f-423e-84cc-e68b6f53e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archb\\AppData\\Local\\Temp\\ipykernel_6592\\3103747979.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efc421b-1df7-4f22-86d1-eaeea65cdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9190315-956e-42c7-9e91-b8ecd4a9e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                                           retriever=retriever,\n",
    "                                                           memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "443560e8-c0c8-4c5b-b8c8-7749a7f008ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archb\\AppData\\Local\\Temp\\ipykernel_6592\\1268749716.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  conversation_chain.run('What is the main topic of the documents ? ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The main topic of the documents appears to be knowledge representation languages and related concepts, as indicated by the mention of sections discussing representation languages and the structure of the book intended for educational purposes in the field of anthropology, psychology, and neuroscience.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.run('What is the main topic of the documents ? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd990048-c66d-4996-ae99-9ad71255bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "937829c6-69bd-4daf-8d4c-3439d6f3da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr \n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0eb2f7b-13e8-4db8-acf4-bcfd0eed13e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "But now I have a story to tell. It’s an important story—one\n",
      "that every engineer and software developer should hear. I’m\n",
      "not entirely satisfied with the way others have told it, so I\n",
      "wrote the book that I wish I had had when I was learning the\n",
      "craft. It starts with the basics and leads you on a journey\n",
      "to the heights of ML and AI. By the end, you’ll understand\n",
      "\n",
      "a constant source of inspiration. Their commitment to AI advancements made my experience of reviewing \n",
      "this book insightful and enriching. Special thanks to my family for their ongoing encouragement throughout \n",
      "this journey.\n",
      "\n",
      "engineer.\n",
      "• Anyone wanting to better understand AI’s capabilities and limitations, and how\n",
      "it might affect different roles.\n",
      "I love getting to the bottom of things, so some sections dive a bit deeper into the tech‐\n",
      "nical side. While many early readers like the detail, it might not be for everyone. I’ll\n",
      "give you a heads-up before things get too technical. Feel free to skip ahead if it feels a\n",
      "little too in the weeds!\n",
      "Preface \n",
      "| \n",
      "xv\n",
      "\n",
      "This is the definitive segue into AI engineering from one of the greats of ML engineering!\n",
      "Chip has seen through successful projects and careers at every stage of a company and\n",
      "for the first time ever condensed her expertise for new AI Engineers entering the field.\n",
      "—swyx, Curator, AI.Engineer\n",
      "AI Engineering is a practical guide that provides the most up-to-date information on AI\n",
      "development, making it approachable for novice and expert leaders alike. This book\n",
      "is an essential resource for anyone looking to build robust and scalable AI systems.\n",
      "—Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks\n",
      "AI Engineering is a comprehensive guide that serves as an essential reference\n",
      "for both understanding and implementing AI systems in practice.\n",
      "—Han Lee, Director—Data Science, Moody’s\n",
      "AI Engineering is an essential guide for anyone building software with Generative AI!\n",
      "It demystifies the technology, highlights the importance of evaluation, and shares\n",
      "Human: Who wrote the book AI engineering ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Debugging why the llm doesn't know the answer despite being fed into the vector store\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "llm = ChatOpenAI(temperature=0.7, model='gpt-4o-mini')\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "retriever = vectordb.as_retriever()\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = 'Who wrote the book AI engineering ?'\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result['answer']\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f4109-c467-42da-902c-f93dfbe2d4a7",
   "metadata": {},
   "source": [
    "* The llm didn't know the answer because it wasn't retrieving enough information chunks\n",
    "* The retriever is an abstraction over the VectorStore that will be used during RAG; \"k\" is how many chunks to use\n",
    "* Putting all together again, and now everytime we invoke the `conversation_chain` it will retrieve **500** chunks from the VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff1c20-43e5-441c-9395-326eaadca929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c332800-6d49-4acd-977c-03ef0cd799d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":500})\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c69c0d42-439a-44a2-a19c-5dabd83e5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c197c4-19d2-4e12-908f-4d51c0791c46",
   "metadata": {},
   "source": [
    "* Problem solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676e947-677e-4ca3-972c-42177b5f3401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
